# Text Spotting

This project implements the MapTextPipeline from MapReader,[^1] designed to detect and extract text from maps, and Loghi,[^2] to transcribe the spotted early modern labels on these maps. 

## Overview

We implement a two-step pipeline for text spotting and transcription:
1. Text spotting - identifying regions of the map that contain text
2. Transcription - converting the detected text regions into machine-readable characters
   1. The MapTextPipeline from MapReader produces a transcription, but this is far from perfect on our historical maps with handwritten text.
   2. Loghi's HTR model is used to transcribe the detected text regions. This model is also used in the GLOBALISE project[^3] and works well with early modern handwriting.
 
The results are saved as AnnotationPage for integration with IIIF (International Image Interoperability Framework) standards.

## Prerequisites

- Docker
- NVIDIA GPU (recommended)
- NVIDIA Container Toolkit (for GPU support)

## Installation

Build the image for textspotting:

```bash
$ docker build -t necessary_reunions_textspotting:latest .
```

## Usage

### Textspotting

#### Running the Container

To run the container, mount your local directories for `images` and `results`. This allows the container to access your map images and save the results back to your host system.

Use `--gpus all` if you have a fancy GPU. Leave this out if you want to run on CPU.

```bash
$ docker run -it --rm \
    -v /data/globalise/maps/necessary_reunions/textspotting/images:/home/mapreader/images \
    -v /data/globalise/maps/necessary_reunions/textspotting/results:/home/mapreader/results \
    --runtime=nvidia --gpus device=2 \
    necessary_reunions_textspotting:latest \
    bash
```

#### Processing an Image

Once inside the container, run the text spotting script:

```bash
$ python spot_text.py images/NL-HaNA_4.VELH_156.2.12.jpg
```

Or, for all images in a directory:

```bash
$ for file in images/*; do python spot_text.py $file; done
```

#### Output

Results will be saved to the `results` directory as AnnotationPage, with the same filename as the input image but with a `.json` extension. The target canvas id is generated by prepending `canvas:` to the filename (without the extension). The output is structured as follows:

```json-ld
{
  "@context": "http://iiif.io/api/presentation/3/context.json",
  "type": "AnnotationPage",
  "items": [
    {
      "@context": "http://www.w3.org/ns/anno.jsonld",
      "id": "a88a10ee-bd72-4df1-a97b-2589ceeb0efe",
      "type": "Annotation",
      "motivation": "textspotting",
      "body": [
        {
          "type": "TextualBody",
          "value": "2",
          "format": "text/plain",
          "purpose": "supplementing",
          "generator": {
            "id": "https://github.com/maps-as-data/MapTextPipeline",
            "type": "Software"
          }
        }
      ],
      "target": {
        "source": "canvas:NL-HaNA_4.VELH_156.2.12",
        "selector": {
          "type": "SvgSelector",
          "value": "<svg xmlns=\"http://www.w3.org/2000/svg\"><polygon points=\"1011,3759 1012,3759 1013,3759 1014,3759 1015,3759 1015,3759 1016,3759 1017,3759 1018,3758 1019,3758 1019,3758 1020,3758 1020,3758 1021,3757 1021,3757 1022,3757 1022,3756 1022,3755 1022,3754 1022,3752 1023,3751 1023,3751 1023,3747 1023,3744 1023,3743 1023,3744 1022,3742 1022,3741 1022,3740 1022,3739 1022,3739 1022,3738 1022,3738 1022,3738 1021,3738 1021,3738 1019,3739 1019,3739 1019,3739 1018,3739 1017,3739 1017,3739 1016,3738 1015,3739 1015,3738 1014,3737 1010,3760 1011,3759 1011,3759\"/></svg>"
        },
        "generator": {
          "id": "https://github.com/maps-as-data/MapTextPipeline",
          "type": "Software"
        }
      }
    },
    ... and the rest
  ]
}
```

### Transcribing

TODO: Loghi

## References

[^1]: McDonough, K., Beelen, K., Wilson, D. C., & Wood, R. (2024). Reading Maps at a Distance: Texts on Maps as New Historical Data. _Imago Mundi, 76_(2), 296-307.

[^2]: Van Koert, R., Klut, S., Koornstra, T., Maas, M., & Peters, L. (2024, August). Loghi: An end-to-end framework for making historical documents machine-readable. In _International Conference on Document Analysis and Recognition_ (pp. 73-88). Cham: Springer Nature Switzerland.

[^3]: Petram, L., & van Rossum, M. (2022). Transforming historical research practicesâ€“a digital infrastructure for the VOC archives (GLOBALISE). _International journal of maritime history, 34_(3), 494-502.
```