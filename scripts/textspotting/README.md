# Text spotting and transcription for early modern maps

This project implements the MapTextPipeline from MapReader,[^1] designed to detect and extract text from maps, and Loghi,[^2] to transcribe the spotted early modern labels on these maps. 

- [Text spotting and transcription for early modern maps](#text-spotting-and-transcription-for-early-modern-maps)
  - [Overview](#overview)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Usage](#usage)
    - [Textspotting](#textspotting)
      - [Running the Container](#running-the-container)
      - [Processing an Image](#processing-an-image)
      - [Output](#output)
    - [Transcribing](#transcribing)
      - [Cutting out the recognized text regions](#cutting-out-the-recognized-text-regions)
      - [Feeding the cutouts to Loghi](#feeding-the-cutouts-to-loghi)
      - [Integrating the results in the AnnotationPage](#integrating-the-results-in-the-annotationpage)


## Overview

We implement a two-step pipeline for text spotting and transcription:
1. Text spotting - identifying regions of the map that contain text
2. Transcription - converting the detected text regions into machine-readable characters
   1. The MapTextPipeline from MapReader produces a transcription, but this is far from perfect on our historical maps with handwritten text.
   2. Loghi's HTR model is used to transcribe the detected text regions. This model is also used in the GLOBALISE project[^3] and works well with early modern handwriting.
 
The results are saved as AnnotationPage for integration with IIIF (International Image Interoperability Framework) standards.

## Prerequisites

- Your favourite container runtime (e.g Docker)
- NVIDIA GPU (recommended)
- NVIDIA Container Toolkit (for GPU support)

## Installation

Build the image for textspotting:

```bash
docker build -t necessary_reunions_textspotting:latest .
```

## Usage

### Textspotting

#### Running the Container

To run the container, mount your local directories for `images` and `results`. This allows the container to access your map images and save the results back to your host system.

Use `--gpus all` if you have a fancy GPU. Leave this out if you want to run on CPU.

```bash
docker run -it --rm \
    -v /data/globalise/maps/necessary_reunions/textspotting/images:/home/mapreader/images \
    -v /data/globalise/maps/necessary_reunions/textspotting/results:/home/mapreader/results \
    --runtime=nvidia --gpus device=2 \
    necessary_reunions_textspotting:latest \
    bash
```

#### Processing an Image

Once inside the container, run the text spotting script:

```bash
python spot_text.py images/NL-HaNA_4.VELH_156.2.12.jpg
```

Or, for all images in a directory:

```bash
for file in images/*; do python spot_text.py $file; done
```

#### Output

Results will be saved to the `results` directory as AnnotationPage, with the same filename as the input image but with a `.json` extension. The target canvas id is generated by prepending `canvas:` to the filename (without the extension). The output is structured as follows:

```json
{
  "@context": "http://iiif.io/api/presentation/3/context.json",
  "type": "AnnotationPage",
  "items": [
    {
      "@context": "http://www.w3.org/ns/anno.jsonld",
      "id": "a88a10ee-bd72-4df1-a97b-2589ceeb0efe",
      "type": "Annotation",
      "motivation": "textspotting",
      "body": [
        {
          "type": "TextualBody",
          "value": "2",
          "format": "text/plain",
          "purpose": "supplementing",
          "generator": {
            "id": "https://github.com/maps-as-data/MapTextPipeline",
            "type": "Software"
          }
        }
      ],
      "target": {
        "source": "canvas:NL-HaNA_4.VELH_156.2.12",
        "selector": {
          "type": "SvgSelector",
          "value": "<svg xmlns=\"http://www.w3.org/2000/svg\"><polygon points=\"1011,3759 1012,3759 1013,3759 1014,3759 1015,3759 1015,3759 1016,3759 1017,3759 1018,3758 1019,3758 1019,3758 1020,3758 1020,3758 1021,3757 1021,3757 1022,3757 1022,3756 1022,3755 1022,3754 1022,3752 1023,3751 1023,3751 1023,3747 1023,3744 1023,3743 1023,3744 1022,3742 1022,3741 1022,3740 1022,3739 1022,3739 1022,3738 1022,3738 1022,3738 1021,3738 1021,3738 1019,3739 1019,3739 1019,3739 1018,3739 1017,3739 1017,3739 1016,3738 1015,3739 1015,3738 1014,3737 1010,3760 1011,3759 1011,3759\"/></svg>"
        },
        "generator": {
          "id": "https://github.com/maps-as-data/MapTextPipeline",
          "type": "Software"
        }
      }
    },
    ... and the rest
  ]
}
```

### Transcribing

#### Cutting out the recognized text regions

The next step is to cut out the recognized text regions from the original images. This is done using the `extract_snippets.py` script. It requires the original map images and the AnnotationPage JSON files generated in the previous step. This script will create cropped images of the detected text regions and save them in a specified output folder.

The script converts the SVG polygons from the annotations back into paths, which are then used to create bounding boxes for cropping. The script also handles rotation and alignment of the cropped images to ensure that the text is horizontally aligned.

Besides the cropped images, the script generates a `lines.txt` file that lists the paths of all saved snippets for each processed image. This file can be used to feed the cutouts into Loghi for transcription (next step)

To cut out the recognized text regions from the original image, run the following command, replacing `<image_folder>`, `<ap_folder>`, and `<snippet_folder>` with the appropriate paths to:
- `image_folder`: The folder containing the original map images, downloaded by the `download_images.py` script.
- `ap_folder`: The folder containing the AnnotationPage files generated by the text spotting step, the `results` folder
- `snippet_folder`: The folder where you want to save the cutout images

Make sure that these are fully qualified paths, as these will end up in the `lines.txt` file.

```bash
python extract_snippets.py <image_folder> <ap_folder> <snippet_folder>
```


#### Feeding the cutouts to Loghi

The transcription of the cutouts is done using Loghi, through the GLOBALISE HTR Model. This model is trained on a large dataset of historical documents and is capable of recognizing handwritten text with high accuracy.

Download the GLOBALISE model from https://hdl.handle.net/10622/X2JZYY (download all files and unzip them), and change these environment variables:
- `HTRLOGHIMODEL`: The path to the model directory
- `SNIPPETSDIR`: The path to the directory where the cutouts are saved

```bash
tmpdir=$(mktemp -d)
HTRLOGHIMODEL="/home/leon/Documents/GLOBALISE/necessary-reunions/scripts/textspotting/model/generic_new_17_2023_05_25_4channel_globalise2"
SNIPPETSDIR="/home/leon/Documents/GLOBALISE/necessary-reunions/scripts/textspotting/snippets"
LOGHIDIR="$(dirname "${HTRLOGHIMODEL}")"

for folder in $(ls $SNIPPETSDIR); do
    DATA="$SNIPPETSDIR/$folder"

    docker run --gpus device=0 -u $(id -u "${USER}"):$(id -g "${USER}") --rm -m 32000m --shm-size 10240m -ti \
        -v /tmp:/tmp \
        -v "$tmpdir":"$tmpdir" \
        -v "$LOGHIDIR":"$LOGHIDIR" \
        -v "$DATA":/data \
        loghi/docker.htr:2.2.13 \
            bash -c "LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 python3 /src/loghi-htr/src/main.py \
            --model $HTRLOGHIMODEL  \
            --batch_size 64 \
            --inference_list /data/lines.txt \
            --results_file /data/results.tsv \
            --gpu 0 \
            --output $tmpdir/output/ \
            --beam_width 1 " | tee -a "$tmpdir"/log.txt ;
done
```

The transcriptions will be saved in a `results.tsv` file in the same directory as the cutouts. The file will contain the paths of the cutouts and their corresponding transcriptions, including a confidence score for each transcription.

#### Integrating the results in the AnnotationPage

Finally, the results from Loghi can be integrated in the Web Annotations that were generated in the first step. This is done in the `integrate_htr_results.py` script that adds an extra textual body with the annotation result of Loghi to the existing annotation. The annotation is removed when no result is found for a specific cutout.

Example body:
```json
{
  "type": "TextualBody",
  "value": "kochin",
  "format": "text/plain",
  "purpose": "supplementing",
  "generator": {
    "id": "https://hdl.handle.net/10622/X2JZYY",
    "type": "Software",
    "label": "GLOBALISE Loghi Handwritten Text Recognition Model - August 2023"
  }
}
```

[^1]: McDonough, K., Beelen, K., Wilson, D. C., & Wood, R. (2024). Reading Maps at a Distance: Texts on Maps as New Historical Data. _Imago Mundi, 76_(2), 296-307.

[^2]: Van Koert, R., Klut, S., Koornstra, T., Maas, M., & Peters, L. (2024, August). Loghi: An end-to-end framework for making historical documents machine-readable. In _International Conference on Document Analysis and Recognition_ (pp. 73-88). Cham: Springer Nature Switzerland.

[^3]: Petram, L., & van Rossum, M. (2022). Transforming historical research practicesâ€“a digital infrastructure for the VOC archives (GLOBALISE). _International journal of maritime history, 34_(3), 494-502.
```